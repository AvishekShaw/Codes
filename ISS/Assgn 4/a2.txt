===========================================
DS221: INTRODUCTION TO SCALABLE SYSTEMS
===========================================
POSTED: 27 NOV, 2019
DUE DATE: 8 DEC, 2019, 11:59PM
POINTS: 50 points
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You will be using MovieLens (ML) data related to movies for performing several simple analytics using Apache Spark [1]. The CSV dataset is posted in HDFS of the turing cluster at [2], and described in detail at [3]. There are small and full versions. You can test your code on the small dataset on the head node. But your assignments must be completed and results reported for the full dataset on the compute cluster. The files you will be using are ratings.csv, tags.csv, movies.csv, and genome-scores.csv.

Answer the following questions by writing a Spark Python script for each using Spark 2.1 over the RDD API [3] and Python 2.7. Do NOT use dataframes or datasets or other advanced Spark features. The problem should be solved for the most part using Spark APIs directly, with *minimal* post processing in the Python driver code if necessary. In all such cases of processing in the driver code, the data size must have been reduced to under 1000 items or 1MB. Each problem should be a separate Python script file. Your driver code must print exactly 1 line of output as the answer for each task.

1) Which movie genres have the most and least number of ratings? (use ratings.csv, movies.csv)
2) Which genres were the most common for movies in the 1990's (1990-1999) and 2000's (2000-2009), normalized by the number of movies in that decade? (use movies.csv, tags.csv)
3) What are the first, second and third quartile values of the ratings given by users to the genre "Animation" and the genre "Sci-Fi"? (use ratings.csv, movies.csv)
4) What are the most and least likely pair of tags and genres to co-occur together? (use movies.csv, tags.csv)
5) Given a movie ID, find the top 5 other movies whose genome scores have the closest and farthest Euclidian distance with this, and print their IDs and names. (use genome-scores.csv and movies.csv)


HINTS:
*) Some of the Spark transforms you will likely need are: map, intersection, reduce, distinct, filter, zipWithIndex, sortBy, lookup, flatMap, reduceByKey, join, groupByKey, sortByKey, 
*) Some of the Spark actions you will likely need are: count, max, min, take
*) Some of the Python operations you will likely need are: split, int, float, len, find, partition, rfind, replace, rpartition, zip, map, reduce, join, sorted, startswith, sqrt 

REFERENCES:
[1] https://spark.apache.org/docs/2.2.1/rdd-programming-guide.html, http://spark.apache.org/docs/2.1.1/api/python/pyspark.html
[2] turing cluster, hdfs:///user/simmhan/ml/
[3] http://files.grouplens.org/datasets/movielens/ml-latest-README.html,
"SMALL: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018."
"FULL: 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. Includes tag genome data with 14 million relevance scores across 1,100 tags. Last updated 9/2018.


===========================================
IMPORTANT NOTE: 
1) Do NOT save your code/scripts in the HDFS folder. Assume it is globally readable by others in the cluster. Keep your code only in your turing home directory that is access controlled. If we find any code present in HDFS, you will get 0 points for that problem. Follow ethics at all times.
2) You may  try your scripts for the "small" dataset using the PySpark shell on the head node. Do NOT run your code it for the "full" data on the head node, and instead use the cluster's compute nodes using spark-submit. Those found abusing the cluster resources will have their jobs killed without warning.
3) Please use the existing source data in HDFS under hdfs:///user/simmhan/ml/ and do NOT create additional copies from the web in your HDFS folders.

===========================================
SUBMISSION INSTRUCTIONS
1) Provide a separate Python file for each problem named as "a2_*.py", where '*' is the problem number from 1-5. 
2) Include a text report with name $username.txt that contains for each problem (1) the console outputs for the print statements, (2) the application ID for the spark job run, and (3) time taken for the execution.
2) Tar/Gzip the 5 Python script files and the single report file into a file named $username-a2.tar.gz under a folder with your username, where $username is your login name in the turing cluster. 
3) Email it to siddharthj@IISc.ac.in and simmhan@iisc.ac.in before 11:59PM on Dec 8, 2019. The subject line should be "ds221-a2-$username". Only a single submission will be accepted, and late submissions will not be accepted.
